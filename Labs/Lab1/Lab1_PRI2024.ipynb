{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6541a8c",
   "metadata": {},
   "source": [
    "## PRI Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37f72c5",
   "metadata": {},
   "source": [
    "general imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36d636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import os, os.path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c6de8",
   "metadata": {},
   "source": [
    "*sklearn*, *nltk* and *whoosh* imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e54be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72117761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh import index, fields, qparser, scoring, matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb1ab38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, nltk.stem\n",
    "from nltk.classify import Senna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba34504",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1) run these commands in case of error:<br>\n",
    "nltk.download('punkt')<br>\n",
    "nltk.download('averaged_perceptron_tagger')<br>\n",
    "nltk.download('maxent_ne_chunker')<br>\n",
    "nltk.download('words')<br>\n",
    "nltk.download('wordnet')\n",
    "2) download senna executable if necessary: https://github.com/baojie/senna/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0490149",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6276ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(path):\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        files += [os.path.join(dirpath,f) for f in filenames]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fe4e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC News Summary\\\\Summaries\\\\business\\\\001.txt',\n",
       " 'BBC News Summary\\\\Summaries\\\\business\\\\002.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_files('BBC News Summary\\\\Summaries')[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa9ea24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British hurdler Sarah Claxton was confident she could win her first major medals at European indoor championships in Madrid. For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form.\n"
     ]
    }
   ],
   "source": [
    "with open('doc1.txt', 'r') as f: print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae72ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week's Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.\n"
     ]
    }
   ],
   "source": [
    "with open('doc2.txt', 'r') as f: print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eccaa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_counts_simple(filename):\n",
    "    result = defaultdict(int)\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            for token in line.split():\n",
    "                result[token] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8916004",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer() #optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d76f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_counts_nltk(filename):\n",
    "    result = defaultdict(int)\n",
    "    with open(filename, 'r') as f:\n",
    "        for sentence in nltk.sent_tokenize(f.read()):\n",
    "            for token in nltk.word_tokenize(sentence):\n",
    "                result[lemmatizer.lemmatize(token)] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405c88ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'British': 1, 'hurdler': 1, 'Sarah': 1, 'Claxton': 2, 'was': 1, 'confident': 1, 'she': 1, 'could': 2, 'win': 1, 'her': 2, 'first': 2, 'major': 1, 'medals': 1, 'at': 1, 'European': 1, 'indoor': 1, 'championships': 1, 'in': 2, 'Madrid.': 1, 'For': 1, 'the': 2, 'time,': 1, 'has': 1, 'only': 1, 'been': 1, 'preparing': 1, 'for': 1, 'a': 1, 'campaign': 1, 'over': 1, 'hurdles': 1, '-': 1, 'which': 1, 'explain': 1, 'leap': 1, 'form.': 1})\n",
      "defaultdict(<class 'int'>, {'British': 1, 'hurdler': 1, 'Sarah': 1, 'Claxton': 2, 'wa': 1, 'confident': 1, 'she': 1, 'could': 2, 'win': 1, 'her': 2, 'first': 2, 'major': 1, 'medal': 1, 'at': 1, 'European': 1, 'indoor': 1, 'championship': 1, 'in': 2, 'Madrid': 1, '.': 2, 'For': 1, 'the': 2, 'time': 1, ',': 1, 'ha': 1, 'only': 1, 'been': 1, 'preparing': 1, 'for': 1, 'a': 1, 'campaign': 1, 'over': 1, 'hurdle': 1, '-': 1, 'which': 1, 'explain': 1, 'leap': 1, 'form': 1})\n"
     ]
    }
   ],
   "source": [
    "print(token_counts_simple('doc1.txt'))\n",
    "print(token_counts_nltk('doc1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb3678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syntactic_class(filename):\n",
    "    result = defaultdict(int)\n",
    "    with open(filename, 'r') as f:\n",
    "        for sentence in nltk.sent_tokenize(f.read()):\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "            for word, tag in nltk.pos_tag(tokens):\n",
    "                result[tag] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fe31ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'JJ': 6, 'NN': 6, 'NNP': 4, 'VBD': 1, 'PRP': 1, 'MD': 2, 'VB': 2, 'PRP$': 2, 'NNS': 3, 'IN': 6, '.': 2, 'DT': 3, ',': 1, 'VBZ': 1, 'RB': 1, 'VBN': 1, 'VBG': 1, ':': 1, 'WDT': 1})\n"
     ]
    }
   ],
   "source": [
    "print(count_syntactic_class('doc1.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "422b479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def senna(filename):\n",
    "    res = []\n",
    "    pipeline = Senna('senna-master', ['pos','chk','ner'])\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            for tags in pipeline.tag(line.split()):\n",
    "                res.append((tags['word'], tags['chk'], tags['ner'], tags['pos']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "005c8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('British', 'B-NP', 'B-MISC', 'JJ'), ('hurdler', 'I-NP', 'O', 'NN'), ('Sarah', 'I-NP', 'B-PER', 'NNP'), ('Claxton', 'I-NP', 'I-PER', 'NNP'), ('was', 'B-VP', 'O', 'VBD'), ('confident', 'B-ADJP', 'O', 'JJ'), ('she', 'B-NP', 'O', 'PRP'), ('could', 'B-VP', 'O', 'MD'), ('win', 'I-VP', 'O', 'VB'), ('her', 'B-NP', 'O', 'PRP$'), ('first', 'I-NP', 'O', 'JJ'), ('major', 'I-NP', 'O', 'JJ'), ('medals', 'I-NP', 'O', 'NNS'), ('at', 'B-PP', 'O', 'IN'), ('European', 'B-NP', 'B-MISC', 'JJ'), ('indoor', 'I-NP', 'O', 'JJ'), ('championships', 'I-NP', 'O', 'NNS'), ('in', 'B-PP', 'O', 'IN'), ('Madrid.', 'B-NP', 'B-MISC', 'NNP'), ('For', 'B-PP', 'I-MISC', 'IN'), ('the', 'B-NP', 'O', 'DT'), ('first', 'I-NP', 'O', 'JJ'), ('time,', 'I-NP', 'O', 'NN'), ('Claxton', 'I-NP', 'B-ORG', 'NNP'), ('has', 'B-VP', 'O', 'VBZ'), ('only', 'I-VP', 'O', 'RB'), ('been', 'I-VP', 'O', 'VBN'), ('preparing', 'I-VP', 'O', 'VBG'), ('for', 'B-PP', 'O', 'IN'), ('a', 'B-NP', 'O', 'DT'), ('campaign', 'I-NP', 'O', 'NN'), ('over', 'B-PP', 'O', 'IN'), ('the', 'B-NP', 'O', 'DT'), ('hurdles', 'I-NP', 'O', 'NNS'), ('-', 'O', 'O', ':'), ('which', 'B-NP', 'O', 'WDT'), ('could', 'B-VP', 'O', 'MD'), ('explain', 'I-VP', 'O', 'VB'), ('her', 'B-NP', 'O', 'PRP$'), ('leap', 'I-NP', 'O', 'NN'), ('in', 'B-PP', 'O', 'IN'), ('form.', 'B-NP', 'O', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(senna('doc1.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b6bf5",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "992fa4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_vectorizer(filenames, tfidf=False):\n",
    "    documents = []\n",
    "    vectorizer = text.TfidfVectorizer() if tfidf else text.CountVectorizer()\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as f: documents.append(f.read())\n",
    "    data = vectorizer.fit_transform(documents)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    return data, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc034d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>60m</th>\n",
       "      <th>and</th>\n",
       "      <th>at</th>\n",
       "      <th>athlete</th>\n",
       "      <th>been</th>\n",
       "      <th>birmingham</th>\n",
       "      <th>born</th>\n",
       "      <th>british</th>\n",
       "      <th>but</th>\n",
       "      <th>campaign</th>\n",
       "      <th>...</th>\n",
       "      <th>trailing</th>\n",
       "      <th>translate</th>\n",
       "      <th>was</th>\n",
       "      <th>week</th>\n",
       "      <th>which</th>\n",
       "      <th>win</th>\n",
       "      <th>won</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   60m  and  at  athlete  been  birmingham  born  british  but  campaign  ...  \\\n",
       "0    0    0   1        0     1           0     0        1    0         1  ...   \n",
       "1    1    1   1        1     0           1     1        0    1         0  ...   \n",
       "\n",
       "   trailing  translate  was  week  which  win  won  world  year  years  \n",
       "0         0          0    1     0      1    1    0      0     0      0  \n",
       "1         1          1    0     1      0    0    1      1     1      1  \n",
       "\n",
       "[2 rows x 74 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, features = sklearn_vectorizer(['doc1.txt','doc2.txt'],False)\n",
    "pd.DataFrame(np.array(data.toarray()),columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d62f65a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>60m</th>\n",
       "      <th>and</th>\n",
       "      <th>at</th>\n",
       "      <th>athlete</th>\n",
       "      <th>been</th>\n",
       "      <th>birmingham</th>\n",
       "      <th>born</th>\n",
       "      <th>british</th>\n",
       "      <th>but</th>\n",
       "      <th>campaign</th>\n",
       "      <th>...</th>\n",
       "      <th>trailing</th>\n",
       "      <th>translate</th>\n",
       "      <th>was</th>\n",
       "      <th>week</th>\n",
       "      <th>which</th>\n",
       "      <th>win</th>\n",
       "      <th>won</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.110240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.154939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.084241</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.118397</td>\n",
       "      <td>0.118397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        60m       and        at   athlete      been  birmingham      born  \\\n",
       "0  0.000000  0.000000  0.110240  0.000000  0.154939    0.000000  0.000000   \n",
       "1  0.118397  0.118397  0.084241  0.118397  0.000000    0.118397  0.118397   \n",
       "\n",
       "    british       but  campaign  ...  trailing  translate       was      week  \\\n",
       "0  0.154939  0.000000  0.154939  ...  0.000000   0.000000  0.154939  0.000000   \n",
       "1  0.000000  0.118397  0.000000  ...  0.118397   0.118397  0.000000  0.118397   \n",
       "\n",
       "      which       win       won     world      year     years  \n",
       "0  0.154939  0.154939  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.118397  0.118397  0.118397  0.118397  \n",
       "\n",
       "[2 rows x 74 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, features = sklearn_vectorizer(['doc1.txt','doc2.txt'],True)\n",
    "pd.DataFrame(np.array(data.toarray()),columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3db03ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.72139823],\n",
       "       [0.72139823, 0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.pairwise_distances(data, metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655605e",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab582181",
   "metadata": {},
   "source": [
    "**IMPT**: the most flexible way of answering the PRI project is by implementing an inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "931df1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(filenames):\n",
    "    index = defaultdict(list)\n",
    "    for i, file in enumerate(filenames):\n",
    "        token_counts = token_counts_nltk(file)\n",
    "        for word in token_counts:\n",
    "            index[word].append((i,token_counts[word]))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c16aa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'British': [(0, 1)], 'hurdler': [(0, 1)], 'Sarah': [(0, 1)], 'Claxton': [(0, 2), (1, 2)], 'wa': [(0, 1)], 'confident': [(0, 1)], 'she': [(0, 1)], 'could': [(0, 2)], 'win': [(0, 1)], 'her': [(0, 2), (1, 1)], 'first': [(0, 2)], 'major': [(0, 1)], 'medal': [(0, 1), (1, 1)], 'at': [(0, 1), (1, 1)], 'European': [(0, 1), (1, 1)], 'indoor': [(0, 1)], 'championship': [(0, 1)], 'in': [(0, 2), (1, 2)], 'Madrid': [(0, 1)], '.': [(0, 2), (1, 3)], 'For': [(0, 1)], 'the': [(0, 2), (1, 6)], 'time': [(0, 1), (1, 1)], ',': [(0, 1), (1, 2)], 'ha': [(0, 1), (1, 2)], 'only': [(0, 1)], 'been': [(0, 1)], 'preparing': [(0, 1)], 'for': [(0, 1), (1, 1)], 'a': [(0, 1)], 'campaign': [(0, 1)], 'over': [(0, 1)], 'hurdle': [(0, 1), (1, 1)], '-': [(0, 1)], 'which': [(0, 1)], 'explain': [(0, 1)], 'leap': [(0, 1)], 'form': [(0, 1)], 'won': [(1, 1)], 'national': [(1, 1)], '60m': [(1, 1)], 'title': [(1, 1)], 'past': [(1, 1)], 'three': [(1, 1)], 'year': [(1, 2)], 'but': [(1, 1)], 'struggled': [(1, 1)], 'to': [(1, 2)], 'translate': [(1, 1)], 'domestic': [(1, 1)], 'success': [(1, 1)], 'international': [(1, 1)], 'stage': [(1, 1)], 'Now': [(1, 1)], 'Scotland-born': [(1, 1)], 'athlete': [(1, 1)], 'owns': [(1, 1)], 'equal': [(1, 1)], 'fifth-fastest': [(1, 1)], 'world': [(1, 1)], 'this': [(1, 1)], 'And': [(1, 1)], 'last': [(1, 1)], 'week': [(1, 1)], \"'s\": [(1, 1)], 'Birmingham': [(1, 1)], 'Grand': [(1, 1)], 'Prix': [(1, 1)], 'left': [(1, 1)], 'favourite': [(1, 1)], 'Russian': [(1, 1)], 'Irina': [(1, 1)], 'Shevchenko': [(1, 1)], 'trailing': [(1, 1)], 'sixth': [(1, 1)], 'spot': [(1, 1)]})\n"
     ]
    }
   ],
   "source": [
    "myindex = inverted_index(['doc1.txt','doc2.txt'])\n",
    "print(myindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3aa7d3",
   "metadata": {},
   "source": [
    "## Exercises 4-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "638f3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_model(terms, index, docs=set()):\n",
    "    if len(terms)==0: return docs\n",
    "    term = lemmatizer.lemmatize(terms.pop(0))\n",
    "    if len(docs)==0:\n",
    "        for doc in index[term]: docs.add(doc[0])\n",
    "        return boolean_model(terms, index, docs)\n",
    "    result = set()\n",
    "    for doc in index[term]:\n",
    "        if doc[0] in docs: result.add(doc[0])\n",
    "    return boolean_model(terms, index, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "589d15b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "{1}\n"
     ]
    }
   ],
   "source": [
    "print(boolean_model(['medal'], myindex, set()))\n",
    "print(boolean_model(['medal','spot'], myindex, set()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f77972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_model(terms, index):\n",
    "    result = defaultdict(int)\n",
    "    for term in terms:\n",
    "        for doc in index[term]:\n",
    "            result[doc[0]]+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3db13d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 1, 1: 2})\n"
     ]
    }
   ],
   "source": [
    "print(TF_model(['medal','spot'], myindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed07fcd",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef38fc0",
   "metadata": {},
   "source": [
    "**IMPT**: you can use whoosh for your PRI project, yet keep in mind that the API may limit your degree of freedom.<br>You can rewrite sources, such as *whoosh.scoring.py*, if you need to customize your vector spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a157922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whoosh_indexing(filenames):\n",
    "    if not os.path.exists('index_ir'): os.mkdir('index_ir')\n",
    "    schema = fields.Schema(id=fields.NUMERIC(stored=True), content=fields.TEXT)\n",
    "    ix = index.create_in(\"index_ir\", schema)\n",
    "    writer = ix.writer()\n",
    "    for identifier, file in enumerate(filenames):\n",
    "        with open(file, 'r') as f:\n",
    "            writer.add_document(id=identifier, content=f.read())\n",
    "    writer.commit()\n",
    "    return ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae3f484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = whoosh_indexing(['doc1.txt','doc2.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2c5c023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#docs: 2\n",
      "CF(has): 3.0\n",
      "DF(has): 2\n",
      "TF_{has,0}: 1\n",
      "TF_{has,1}: 2\n"
     ]
    }
   ],
   "source": [
    "with ix.searcher().reader() as ixreader:\n",
    "    print('#docs:', ixreader.doc_count_all())\n",
    "    print('CF(has):', ixreader.frequency(fieldname='content',text='has'))\n",
    "    print('DF(has):', ixreader.doc_frequency(fieldname='content',text='has'))\n",
    "    for posting in ixreader.postings(\"content\", \"has\").items_as('frequency'):\n",
    "        print('TF_{has,'+str(posting[0])+'}:',posting[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e7c4b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whosh_query(query):\n",
    "    ix = index.open_dir(\"index_ir\")\n",
    "    with ix.searcher() as searcher:\n",
    "        q = qparser.QueryParser(\"content\", ix.schema, group=qparser.OrGroup).parse(query)\n",
    "        return searcher.search(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87648ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0  score: 0.85080415159161\n",
      "ID: 1  score: 0.776043291106279\n"
     ]
    }
   ],
   "source": [
    "results = whosh_query(\"Claxton\")\n",
    "for doc, score in results.items():\n",
    "    print(\"ID:\",doc,\" score:\",score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
