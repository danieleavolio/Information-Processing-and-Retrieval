\section{Proposed questions}

\section*{Part A: Clustering}

\subsection{Question 1}
\textbf{Do clustering-guided summarization alters the behavior and efficacy of the IR system?}

To answer this question we ran the \textbf{clustering based} algorithm using
the same set of documents used in the \textit{first part of the project}. The
result shows a significative difference between the two approaches.

\begin{figure}[H]
  \centering
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/pr_question_1_part1.png}
    \captionof{figure}{First Part Approach}
    \label{fig:firstpart}
  \end{minipage}%
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/pr_question_1.png}
    \captionof{figure}{Clustered Approach}
    \label{fig:clustered}
  \end{minipage}
\end{figure}

This result is not enough to guarantee that the \textbf{clustering based} approach is
worse than the approach using TFIDF and BM25, because this is based on our
personal implementation of the algorithm. However, it is clear that the
clustering approach is not as effective as the first approach in this case. A
possible way to improve the algorithm could be to consider other selection criteria rather than focusing on the distance from the centroid of each cluster.

\subsection{Question 2}
\textbf{How sentence representations, clustering choices, and rank criteria impact summarization?}

We benchmarked the performance of the clustering algorithm using a set of
metrics:
\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline
    \textbf{Max Clusters} & \textbf{Number of Sentences} & \textbf{Our Metrics}  \\
    \hline
    2                     & 1                            &                       \\
    3                     & 2                            & Cosine sim. (single)  \\
    4                     & 3                            & Cosine sim. (complete)\\
    6                     & 4                            &                       \\
   \hline
  \end{tabular}
\end{table}

We didn't include any \textbf{different representations} because we only used \textbf{TFIDF}. The result are indicating a very low
performance of the algorithm using a specific set of metrics.

\begin{table}[H]

  \begin{adjustwidth}{-0.5in}{-.5in}  
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|} 
  \hline 
  clusters & num\_sent & metric & avg\_prec & avg\_rec & f1 & std\_prec & std\_rec & m\_a\_p \\
  \hline 
  2 & 1 & single & 0.403860 & 0.153016 & 0.221942 & 0.171560 & 0.099392 & 0.484703 \\
  \hline 
  2 & 1 & complete & 0.427703 & 0.162918 & 0.235957 & 0.170426 & 0.096330 & 0.517471 \\
  \hline 
  2 & 2 & single & 0.403429 & 0.230112 & 0.293063 & 0.159069 & 0.134892 & 0.686571 \\
  \hline
  $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\cdots$ \\
  \hline 
  4 & 3 & single & 0.405465 & 0.329316 & 0.363445 & 0.133851 & 0.193184 & 0.666388 \\
  \hline 
  4 & 3 & complete & 0.447709 & 0.503877 & 0.474135 & 0.110074 & 0.217150 & 0.632657 \\
  \hline 
  4 & 4 & single & 0.413300 & 0.397615 & 0.405306 & 0.119253 & 0.205291 & 0.670563 \\
  \hline 
  4 & 4 & complete & 0.448957 & 0.590894 & 0.510239 & 0.095870 & 0.223584 & 0.554584 \\
  \hline
  \end{tabular}
  \caption{Results of the clustering algorithm using different metrics}

  \end{adjustwidth}
  \end{table}

  The complete table can be seen on the notebook. However, we can notice that evaluation scores changes a lot in function of these parameters. In particular, the best setup for this subset of documents is obtained at max\_clusters set to 6 and number of sentences per cluster set to 2, with single metric. 
\subsection{Question 3}
\textbf{ Are anchor sentences (capturing multiple topics) included? And less relevant outlier sen- tences excluded? Justify} \\
Since our algorithm is based on the \textbf{distance from the centroid} of each
cluster to select the sentences, we are not able to handle the \textbf{anchor
  sentences} and the \textbf{outlier sentences}. Thus, we can not give a clear
answer to this question, but a possible way to deal with this problem this could be to consider
the \textbf{distance from the centroid} and the \textbf{distance from the other
  sentences} inside other clusters. Sentences that are \textbf{farther} from the
centroid of the cluster could be very \textbf{relevant} and could be considered
as \textbf{anchor sentences}, since that sentence could be holding cross-cluster informations.

\subsection{Question 4}
\textbf{Given a set of documents, plot the distribution of the number of keywords per document. Are keywords generally dissimilar? If not, how would you tackle this challenge?} \\
For this question we decided to use documents from \textbf{500} to \textbf{700} as range.
The result shows that the distribution of the number of keywords per
document is not uniform.

\begin{figure}[H]
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/keyword_distribution.png}
    \captionof{figure}{Distribution of the number of keywords per document}
    \label{fig:question4_1} 
  \end{minipage}
  \begin{minipage}{.5\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{images/keyword_distribution_ranks.png}
    \captionof{figure}{Top 10 used keywords}
    \label{fig:question4_2}
  \end{minipage}
\end{figure}

The main problem is that the \textbf{keywords} are not \textbf{dissimilar} and
this could be a problem for the \textbf{clustering algorithm}. In fact, we can
see that a lot of keywords are repeated in the documents. A possible way to
tackle this challenge could be to use a different \textbf{keyword extraction} method. Moreover, using multiple documents of the same
category could help to improve the performance of the algorithm since the
keywords are more likely to be repeated in the same category.

\section*{Part B: Supervised IR}
\subsection{Question 1}
\textbf{ Does the incorporation of relevance feedback from ideal extracts significantly impact the performance of the IR system? Hypothesize why is that so.}
Comparing the evaluation of two best models, respectively the unsupervised model from question 3.2 and the supervised RandomForest model, it seems that the supervised IR is better performing in a perceptible way. We can say that even if the actual test sets are different since they have a significant high number of documents and the scores are way too different.


\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\textbf{Model} & \textbf{Avg Precision} & \textbf{Avg Recall} & \textbf{F1 Score} & \textbf{\#docs} \\
Clustering & 0.410742 & 0.304867 & 0.349973 & 200 \\
Random Forest & 0.746500 & 0.790460 & 0.767851 & 443 \\
\end{tabular}
\caption{Performance Metrics of unsupervised best model  and supervised best model}
\label{tab:performance}
\end{table}

From our experience we carefully can say that overall an approach based on a reference target category is usually better performing than an unsupervised model. This obviously stands assuming the high quality of the reference summaries. 

\subsection{Question 2}
\textbf{ Are the learned models able to generalize from one category to another? Justify.}

To answer this question, we treid to train the model using the \textbf{tech category} as a training set, and 
all the other categories as a test set. We used \textit{Random Forest} as a model, since it was the best between the others.
The results show that the model achieved $\approx 0.69$ as it's best performance and $\approx 0.65$ as it's worst performance. 
This result is not bad, but it's not good either. This happens because each category is different from another, and the set of features 
that we are using may not be the best. However, to \textit{effectively test the model}, what we could do is to train another model 
using a training set of the same size as the \textit{tech category training set}, but using a mixture between all the categories.
In this way we could have a better vision of the model's performance in the generalization task. 
The code and the specific result for each category can be found in the \textit{notebook} file.


\subsection{Question 3}
\textbf{Which features appear to be more relevant to the target summarization task? Do sentence- location features aid summarization?}

The most important feature that we found is the score given by the \textit{similarity} between the sentence and the document. To get this 
insight, we used the \textit{SHAP} library that uses, to highlight the importance of each feature, the concept of 
\textit{Shapley Values}, a metric from game theory that given is able to assigns to each agent (our feature) a value that represents the contribution to the model's prediction.
From this analysis, we can say that the \textit{sentence-location} feature contributes to the task, since it is the 4th most important feature in the model.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{images/shap.png}
  \caption{SHAP values for the Random Forest model}
  \label{fig:shap_values}
\end{figure}

\subsection{Question 4}
\textbf{In alternative to the given reference extracts, consider the presence of manual abstractive summaries, can supervised IR be used to explore such feedback? Justify.}
This task might result difficult to be conducted with a machine learning approach, since there is no direct access to a target function for sentences, neither a way to build up a summary from a set of features without human supervision. This second fact makes the only possible approach to summarization being the extractive one. \\
The task of supervised summarization can not be conduced as we did during the part B of this delivery, since we can not construct a dataset that use sentences as features.\\
The abstractive approach is a summarization based on the context of the document, trying to reproduce the same concept in a different shape. This ends up in sentences from the text not being in the summary and, thus, the dataset we have created can not be replicated in the exact same shape. 

A possible solution to the task with abstractive summaries could be to use the deep learning BERT, a bidirectional encoder based on Transformer architecture that is able to transpose the input text into a latent space, in which similar concepts are close to each other. \\
Exploiting this, our suggestion is to use the BERT model to encode both the summary, that serves as an anchor reference as provided ground truth, and the sentence we want to classify. The resulting embeddings can be used in a classifier or to compute other metrics, like distance ones, and then use those features to select sentences based on a criterion. This will still produce an extractive summary.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\textwidth]{images/abstractive.png}
  \caption{Possible architecture using abstractive summaries}
  \label{fig:abstractive}
\end{figure} 

Another approach that can be explored is based on keywords from the summary. This can be divided in two steps: during the first one, we construct a reference set of keywords, extracting them from the whole set of summaries and expanding them with one iteration on a predefined thesaurus, to make sure to have also synonims of relevant words. \\
In the second phase, a sentence can be classified calculating a score which is the rate of number of keywords from the sentence that are present in the summary-extracted ones (also referred as \textbf{keywords batch} from now on) over the number of keywords in the document that contains the sentence, as it follows.

$$score = \frac{|keywordsFromSentenceInBatch|}{|keywordsInDocument|}$$
Note that this formula has the number of keywords in the document as denominator to prefer, among the same document, sentences with a higher keywords "density".\\
Once calculated the score, it is possible to use it to select the sentences that will be part of the summary using a fixed threshold, that can eventually be updated based on a feedback system that collects them from the final user. 
Here it is presented a pseudo code to better explain the algorithm.

\begin{lstlisting}
  keywords_batch = get_keywords_from_summaries()

  for sentence in document:
    score = calculate_score(sentence, document, keywords_batch)
    if score >= threshold:
      summary.add(sentence)
  
  return summary
  
\end{lstlisting}