{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 8**\n",
    "- Daniele Avolio    , ist1111559\n",
    "- Michele Vitale\t, ist1111558\t\n",
    "- Lu√≠s Dias\t        , ist198557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, let's create a `function` to read the `documents` we need to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "\n",
    "def read_files(location:str):\n",
    "    filespath = []  \n",
    "    \n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"): \n",
    "                filespath.append(os.path.join(root, file))  \n",
    "    \n",
    "    return filespath  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_paths = read_files(\"../BBC News Summary/News Articles\")  \n",
    "print(documents_paths[:5])\n",
    "print(f\"The number of documents is {len(documents_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible function to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code, statistics and/def\n",
    "import os\n",
    "import time\n",
    "from whoosh import index, scoring\n",
    "from whoosh.fields import Schema, TEXT, NUMERIC\n",
    "from whoosh.analysis import StandardAnalyzer, StemFilter, LowercaseFilter, StopFilter\n",
    "\n",
    "stoplist = frozenset(\n",
    "    [\n",
    "        \"and\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"an\",\n",
    "        \"as\",\n",
    "        \"at\",\n",
    "        \"have\",\n",
    "        \"in\",\n",
    "        \"yet\",\n",
    "        \"if\",\n",
    "        \"from\",\n",
    "        \"for\",\n",
    "        \"when\",\n",
    "        \"by\",\n",
    "        \"to\",\n",
    "        \"you\",\n",
    "        \"be\",\n",
    "        \"we\",\n",
    "        \"that\",\n",
    "        \"may\",\n",
    "        \"not\",\n",
    "        \"with\",\n",
    "        \"tbd\",\n",
    "        \"a\",\n",
    "        \"on\",\n",
    "        \"your\",\n",
    "        \"this\",\n",
    "        \"of\",\n",
    "        \"us\",\n",
    "        \"will\",\n",
    "        \"can\",\n",
    "        \"the\",\n",
    "        \"or\",\n",
    "        \"are\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def indexing(document_collection, stem=True, stop_words=True):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # It's important to put the stoplist check here because in the constructor\n",
    "    # of the StandardAnalyzer, the stoplist parameter is set automatically to a default\n",
    "    # so if we want to remove it, we need to check it during the construction\n",
    "    analyzer = StandardAnalyzer(stoplist=stoplist if stop_words else None)\n",
    "\n",
    "    if stem:\n",
    "        analyzer = analyzer | StemFilter()\n",
    "\n",
    "    \n",
    "\n",
    "    # print(analyzer)\n",
    "    \n",
    "\n",
    "    schema = Schema(\n",
    "        id=NUMERIC(stored=True),\n",
    "        content=TEXT(\n",
    "            analyzer=analyzer,\n",
    "            stored=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    index_dir = \"indexdirectory\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "\n",
    "    ix = index.create_in(index_dir, schema)\n",
    "\n",
    "    writer = ix.writer()\n",
    "\n",
    "    for doc_id, document in enumerate(document_collection):\n",
    "        with open(document, \"r\") as file:\n",
    "            content = file.read()\n",
    "            writer.add_document(id=doc_id, content=content)\n",
    "\n",
    "    writer.commit()\n",
    "\n",
    "    indexing_time = time.time() - start_time\n",
    "\n",
    "    return ix, indexing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "ix, indexing_time = indexing(documents_paths)\n",
    "\n",
    "print(f\"Indexing time: {indexing_time} seconds\")\n",
    "print(f\"Number of indexed documents: {ix.doc_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    query = QueryParser(\"content\", ix.schema).parse(\"PC\")\n",
    "    results = searcher.search(query, limit=5)\n",
    "\n",
    "    for hit in results:\n",
    "        print(f\"Document id: {hit['id']} document score: {hit.score}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Things to do more:`\n",
    "- Add a `function` that gives statistics about the `documents` (e.g. number of words, number of characters, etc.)\n",
    "- Add a `function` that gives the `frequency` of each word in the `documents` (e.g. word1: 10, word2: 5, etc.)\n",
    "- Somethink else?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be changed. It's better to implement a good BM25 algorithm, and this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import Counter\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "def summarization(document:str, max_sentences:int, max_characters:int, order:bool, ix):\n",
    "\n",
    "    # It's better to tokenize into sentences \n",
    "    sentences = sent_tokenize(document)\n",
    "\n",
    "    # The main idea is to take a sentence and give it a score based on the frequency of its terms\n",
    "    # Then we select the sentences with the highest scores\n",
    "    with ix.searcher() as searcher:\n",
    "        sentence_scores = {}\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            score = 0\n",
    "            for word in sentence.split():\n",
    "                # We use the frequency of the word in the whole collection as a score\n",
    "                score += searcher.frequency(\"content\", word)\n",
    "            sentence_scores[i] = score\n",
    "\n",
    "    # Organize the sentences by their scores\n",
    "            # Uses lambda function to sort the dictionary by value, taking the second element of the tuple\n",
    "    sorted_sentence_scores = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "\n",
    "    summary_sentences = []\n",
    "    summary_length = 0\n",
    "    for i, score in sorted_sentence_scores:\n",
    "        sentence = sentences[i]\n",
    "        if summary_length + len(sentence) > max_characters:\n",
    "            break\n",
    "        summary_sentences.append((i, sentence))\n",
    "        summary_length += len(sentence)\n",
    "        if len(summary_sentences) >= max_sentences:\n",
    "            break\n",
    "\n",
    "    # If order is True, sort the sentences into their original order\n",
    "    if order:\n",
    "        summary_sentences.sort(key=lambda item: item[0])\n",
    "\n",
    "    # Join the sentences together into a single string\n",
    "    summary = \" \".join(sentence for i, sentence in summary_sentences)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Warner said on Friday that it now owns 8% of search-engine Google. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the summarization function\n",
    "document = documents_paths[0]\n",
    "with open(document, \"r\") as file:\n",
    "    content = file.read()\n",
    "    summary = summarization(content, max_sentences=5, max_characters=500, order=True, ix=ix)\n",
    "    print(summary)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and BERT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
