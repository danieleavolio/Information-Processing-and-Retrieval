{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 8**\n",
    "- Daniele Avolio    , ist1111559\n",
    "- Michele Vitale\t, ist1111558\t\n",
    "- Lu√≠s Dias\t        , ist198557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, let's create a `function` to read the `documents` we need to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "\n",
    "def read_files(location:str):\n",
    "    filespath = []  \n",
    "    \n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"): \n",
    "                filespath.append(os.path.join(root, file))  \n",
    "    \n",
    "    return filespath  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../BBC News Summary/News Articles\\\\business\\\\001.txt', '../BBC News Summary/News Articles\\\\business\\\\002.txt', '../BBC News Summary/News Articles\\\\business\\\\003.txt', '../BBC News Summary/News Articles\\\\business\\\\004.txt', '../BBC News Summary/News Articles\\\\business\\\\005.txt']\n",
      "The number of documents is 2225\n"
     ]
    }
   ],
   "source": [
    "documents_paths = read_files(\"../BBC News Summary/News Articles\")  \n",
    "print(documents_paths[:5])\n",
    "print(f\"The number of documents is {len(documents_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible function to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code, statistics and/def\n",
    "import os\n",
    "import time\n",
    "from whoosh import index, scoring\n",
    "from whoosh.fields import Schema, TEXT, NUMERIC\n",
    "from whoosh.analysis import StandardAnalyzer, StemFilter, LowercaseFilter, StopFilter\n",
    "\n",
    "stoplist = frozenset(\n",
    "    [\n",
    "        \"and\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"an\",\n",
    "        \"as\",\n",
    "        \"at\",\n",
    "        \"have\",\n",
    "        \"in\",\n",
    "        \"yet\",\n",
    "        \"if\",\n",
    "        \"from\",\n",
    "        \"for\",\n",
    "        \"when\",\n",
    "        \"by\",\n",
    "        \"to\",\n",
    "        \"you\",\n",
    "        \"be\",\n",
    "        \"we\",\n",
    "        \"that\",\n",
    "        \"may\",\n",
    "        \"not\",\n",
    "        \"with\",\n",
    "        \"tbd\",\n",
    "        \"a\",\n",
    "        \"on\",\n",
    "        \"your\",\n",
    "        \"this\",\n",
    "        \"of\",\n",
    "        \"us\",\n",
    "        \"will\",\n",
    "        \"can\",\n",
    "        \"the\",\n",
    "        \"or\",\n",
    "        \"are\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def indexing(document_collection, stem=True, stop_words=True):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # It's important to put the stoplist check here because in the constructor\n",
    "    # of the StandardAnalyzer, the stoplist parameter is set automatically to a default\n",
    "    # so if we want to remove it, we need to check it during the construction\n",
    "    analyzer = StandardAnalyzer(stoplist=stoplist if stop_words else None)\n",
    "\n",
    "    if stem:\n",
    "        analyzer = analyzer | StemFilter()\n",
    "\n",
    "    \n",
    "\n",
    "    # print(analyzer)\n",
    "    \n",
    "\n",
    "    schema = Schema(\n",
    "        id=NUMERIC(stored=True),\n",
    "        content=TEXT(\n",
    "            analyzer=analyzer,\n",
    "            stored=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    index_dir = \"indexdirectory\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "\n",
    "    ix = index.create_in(index_dir, schema)\n",
    "\n",
    "    writer = ix.writer()\n",
    "\n",
    "    for doc_id, document in enumerate(document_collection):\n",
    "        with open(document, \"r\") as file:\n",
    "            content = file.read()\n",
    "            writer.add_document(id=doc_id, content=content)\n",
    "\n",
    "    writer.commit()\n",
    "\n",
    "    indexing_time = time.time() - start_time\n",
    "\n",
    "    return ix, indexing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 28.83882164955139 seconds\n",
      "Number of indexed documents: 2225\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "ix, indexing_time = indexing(documents_paths)\n",
    "\n",
    "print(f\"Indexing time: {indexing_time} seconds\")\n",
    "print(f\"Number of indexed documents: {ix.doc_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    query = QueryParser(\"content\", ix.schema).parse(\"PC\")\n",
    "    results = searcher.search(query, limit=5)\n",
    "\n",
    "    for hit in results:\n",
    "        print(f\"Document id: {hit['id']} document score: {hit.score}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Things to do more:`\n",
    "- Add a `function` that gives statistics about the `documents` (e.g. number of words, number of characters, etc.)\n",
    "- Add a `function` that gives the `frequency` of each word in the `documents` (e.g. word1: 10, word2: 5, etc.)\n",
    "- Somethink else?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This needs to be changed. It's better to implement a good BM25 algorithm, and this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from whoosh import scoring\n",
    "\n",
    "\n",
    "def summarization(\n",
    "    document: str,\n",
    "    max_sentences: int,\n",
    "    max_characters: int,\n",
    "    order: bool,\n",
    "    ix,\n",
    "    scoring_type: str = \"TF_IDF\",\n",
    "):\n",
    "\n",
    "    # It's better to tokenize into sentences\n",
    "    sentences = sent_tokenize(document)\n",
    "\n",
    "    if scoring_type != \"BERT\":\n",
    "        # The main idea is to take a sentence and give it a score based on the frequency of its terms\n",
    "        # Then we select the sentences with the highest scores\n",
    "        with ix.searcher(\n",
    "            weighting=scoring.TF_IDF() if scoring_type == \"TF_IDF\" else scoring.BM25F(\n",
    "                B=0.75, K1=1.2\n",
    "            )\n",
    "        ) as searcher:\n",
    "            sentence_scores = {}\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                score = 0\n",
    "                for word in sentence.split():\n",
    "                    # We use the frequency of the word in the whole collection as a score\n",
    "                    score += searcher.frequency(\"content\", word)\n",
    "\n",
    "                sentence_scores[i] = score / len(sentences)\n",
    "\n",
    "        # Organize the sentences by their scores\n",
    "        # Uses lambda function to sort the dictionary by value, taking the second element of the tuple\n",
    "        sorted_sentence_scores = sorted(\n",
    "            sentence_scores.items(), key=lambda item: item[1], reverse=True\n",
    "        )\n",
    "\n",
    "        summary_sentences = []\n",
    "        summary_length = 0\n",
    "        for i, score in sorted_sentence_scores:\n",
    "            sentence = sentences[i]\n",
    "            if summary_length + len(sentence) > max_characters:\n",
    "                break\n",
    "            summary_sentences.append((i, sentence))\n",
    "            summary_length += len(sentence)\n",
    "            if len(summary_sentences) >= max_sentences:\n",
    "                break\n",
    "\n",
    "        # If order is True, sort the sentences into their original order\n",
    "        if order:\n",
    "            summary_sentences.sort(key=lambda item: item[0])\n",
    "\n",
    "        # Join the sentences together into a single string\n",
    "        summary = \" \".join(sentence for i, sentence in summary_sentences)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Warner said on Friday that it now owns 8% of search-engine Google. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the summarization function\n",
    "document = documents_paths[0]\n",
    "with open(document, \"r\") as file:\n",
    "    content = file.read()\n",
    "    summary = summarization(content, max_sentences=5, max_characters=500, order=True, ix=ix, scoring_type=\"TF_IDF\")\n",
    "    print(summary)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and BERT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: 2065 Score: 7.881376447520268\n",
      "Id: 2206 Score: 7.249861037345842\n",
      "Id: 1934 Score: 7.1905429154835865\n",
      "Id: 2073 Score: 7.102620146365696\n",
      "Id: 2124 Score: 7.095863705043671\n",
      "Id: 1906 Score: 7.029048064877089\n",
      "Id: 2131 Score: 7.029048064877089\n",
      "Id: 2159 Score: 6.845299733583026\n",
      "Id: 2066 Score: 6.798863188653262\n",
      "Id: 249 Score: 6.795487552532308\n"
     ]
    }
   ],
   "source": [
    "# code, statistics and/or charts here\n",
    "from whoosh.qparser import *\n",
    "\n",
    "\n",
    "# probably querying (ask Rui)\n",
    "def test_model(type: str, query: str, ix):\n",
    "    score = None\n",
    "    if type == \"BM25\":\n",
    "        score = scoring.BM25F(B=0.75, K1=1.2)\n",
    "    elif type == \"TF-IDF\":\n",
    "        score = scoring.TF_IDF()\n",
    "    elif type == \"BERT\":\n",
    "        pass\n",
    "\n",
    "    with ix.searcher(weighting=score) as searcher:\n",
    "        q = QueryParser(\"content\", ix.schema, group=OrGroup).parse(query)\n",
    "        results = searcher.search(q, limit=10)\n",
    "\n",
    "        for r in results:\n",
    "            print(f\"Id: {r['id']} Score: {r.score}\")\n",
    "\n",
    "\n",
    "test_model(\"BM25\", \"PC\", ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\danie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#code, statistics and/or charts here\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "def keyword_extraction(document:str, maximum_keyword:int, ix, **kwargs):\n",
    "\n",
    "    word_tag = nltk.pos_tag(nltk.word_tokenize(document))\n",
    "\n",
    "\n",
    "\n",
    "    # We can filter the words based on their tags keeping only the important ones for keyword extraction\n",
    "    # filter are here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    tags = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "    important_words = [word for word, tag in word_tag if tag in tags]\n",
    "\n",
    "\n",
    "    words_scores = defaultdict(int)\n",
    "\n",
    "    # assess the score of each word at the document level\n",
    "    with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "        for word in important_words:\n",
    "            words_scores[word] = searcher.frequency(\"content\", word)\n",
    "\n",
    "    sorted_words_scores = sorted(words_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    keywords = [word for word in sorted_words_scores[:maximum_keyword]]\n",
    "\n",
    "    return keywords\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 1177.0), ('back', 1039.0), ('firm', 1004.0), ('music', 949.0), ('market', 924.0), ('sale', 738.0), ('part', 634.0), ('deal', 514.0), ('growth', 467.0), ('profit', 333.0)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document = documents_paths[0]\n",
    "with open(document, \"r\") as file:\n",
    "    content = file.read()\n",
    "    keywords = keyword_extraction(content, 10, ix)\n",
    "    print(keywords)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# try this later: https://whoosh.readthedocs.io/en/latest/keywords.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
