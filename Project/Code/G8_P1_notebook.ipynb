{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2023/24: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP 8**\n",
    "- Daniele Avolio    , ist1111559\n",
    "- Michele Vitale\t, ist1111558\t\n",
    "- Lu√≠s Dias\t        , ist198557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) **Indexing** (preprocessing and indexing options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, let's create a `function` to read the `documents` we need to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "\n",
    "def read_files(location:str):\n",
    "    filespath = []  \n",
    "    \n",
    "    for root, dirs, files in os.walk(location):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"): \n",
    "                filespath.append(os.path.join(root, file))  \n",
    "    \n",
    "    return filespath  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../BBC News Summary/News Articles\\\\business\\\\001.txt', '../BBC News Summary/News Articles\\\\business\\\\002.txt', '../BBC News Summary/News Articles\\\\business\\\\003.txt', '../BBC News Summary/News Articles\\\\business\\\\004.txt', '../BBC News Summary/News Articles\\\\business\\\\005.txt']\n",
      "The number of documents is 2225\n"
     ]
    }
   ],
   "source": [
    "documents_paths = read_files(\"../BBC News Summary/News Articles\")  \n",
    "print(documents_paths[:5])\n",
    "print(f\"The number of documents is {len(documents_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible function to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code, statistics and/def\n",
    "import os\n",
    "import time\n",
    "from whoosh import index, scoring\n",
    "from whoosh.fields import Schema, TEXT, NUMERIC\n",
    "from whoosh.analysis import StandardAnalyzer, StemFilter, LowercaseFilter, StopFilter\n",
    "\n",
    "stoplist = frozenset(\n",
    "    [\n",
    "        \"and\",\n",
    "        \"is\",\n",
    "        \"it\",\n",
    "        \"an\",\n",
    "        \"as\",\n",
    "        \"at\",\n",
    "        \"have\",\n",
    "        \"in\",\n",
    "        \"yet\",\n",
    "        \"if\",\n",
    "        \"from\",\n",
    "        \"for\",\n",
    "        \"when\",\n",
    "        \"by\",\n",
    "        \"to\",\n",
    "        \"you\",\n",
    "        \"be\",\n",
    "        \"we\",\n",
    "        \"that\",\n",
    "        \"may\",\n",
    "        \"not\",\n",
    "        \"with\",\n",
    "        \"tbd\",\n",
    "        \"a\",\n",
    "        \"on\",\n",
    "        \"your\",\n",
    "        \"this\",\n",
    "        \"of\",\n",
    "        \"us\",\n",
    "        \"will\",\n",
    "        \"can\",\n",
    "        \"the\",\n",
    "        \"or\",\n",
    "        \"are\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def indexing(document_collection, stem=True, stop_words=True):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # It's important to put the stoplist check here because in the constructor\n",
    "    # of the StandardAnalyzer, the stoplist parameter is set automatically to a default\n",
    "    # so if we want to remove it, we need to check it during the construction\n",
    "    analyzer = StandardAnalyzer(stoplist=stoplist if stop_words else None)\n",
    "\n",
    "    if stem:\n",
    "        analyzer = analyzer | StemFilter()\n",
    "\n",
    "    \n",
    "\n",
    "    print(analyzer)\n",
    "    \n",
    "\n",
    "    schema = Schema(\n",
    "        id=NUMERIC(stored=True),\n",
    "        content=TEXT(\n",
    "            analyzer=analyzer,\n",
    "            stored=True,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    index_dir = \"indexdirectory\"\n",
    "    if not os.path.exists(index_dir):\n",
    "        os.mkdir(index_dir)\n",
    "\n",
    "    # In order to not have lock problems, we need to check if the index exists\n",
    "    # and if we already have one, we need to open it, otherwise we create it\n",
    "    ix = index.create_in(index_dir, schema)\n",
    "\n",
    "    writer = ix.writer()\n",
    "\n",
    "    for doc_id, document in enumerate(document_collection):\n",
    "        with open(document, \"r\") as file:\n",
    "            content = file.read()\n",
    "            writer.add_document(id=doc_id, content=content)\n",
    "\n",
    "    writer.commit()\n",
    "\n",
    "    indexing_time = time.time() - start_time\n",
    "\n",
    "    return ix, indexing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeAnalyzer(RegexTokenizer(expression=re.compile('\\\\w+(\\\\.?\\\\w+)*'), gaps=False), LowercaseFilter(), StemFilter(stemfn=<function stem at 0x000002AA4B0064C0>, lang=None, ignore=frozenset(), cachesize=50000, _stem=<function stem at 0x000002AA4AF69D30>))\n",
      "Indexing time: 14.64324426651001 seconds\n",
      "Number of indexed documents: 2225\n",
      "Document id: 1185 document score: 244.88991237738838\n",
      "\n",
      "\n",
      "\n",
      "Document id: 762 document score: 204.90788586679435\n",
      "\n",
      "\n",
      "\n",
      "Document id: 1275 document score: 189.9146259253216\n",
      "\n",
      "\n",
      "\n",
      "Document id: 862 document score: 168.92406200725972\n",
      "\n",
      "\n",
      "\n",
      "Document id: 1683 document score: 144.93484610090331\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "ix, indexing_time = indexing(documents_paths, stop_words=False)\n",
    "\n",
    "print(f\"Indexing time: {indexing_time} seconds\")\n",
    "print(f\"Number of indexed documents: {ix.doc_count()}\")\n",
    "\n",
    "with ix.searcher(weighting=scoring.TF_IDF()) as searcher:\n",
    "    query = QueryParser(\"content\", ix.schema).parse(\"the\")\n",
    "    results = searcher.search(query, limit=5)\n",
    "\n",
    "    for hit in results:\n",
    "        print(f\"Document id: {hit['id']} document score: {hit.score}\")\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) **Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 IR models (TF-IDF, BM25 and EBRT)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) **Keyword extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D) **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Corpus *D* and summaries *S* description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
