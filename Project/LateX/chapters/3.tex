\section{Proposed questions}
In this section, we answer point-to-point to the questions proposed during the project description.

\subsection{Describe the corpus $D$ and summaries $S$. Are terms uniformly distributed regarding TF-IDF?}
Fixed the x axis on terms, it can be seen by the figures that the plotted
distribution of words remains close to unchanged. The cardinality of terms changes due to the fact that many words are not relevant to the summarization task, so the do not appear on the vocabulary of the summaries $S$. Also, it is important to
note that the y axis has a different scale, due to the smaller cardinality of the summary tokens in respect of the corpus $D$.
\begin{figure}[h]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/dist_D.png}
        \caption{Corpus distribution}
        \label{fig:Corpus}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/dist_S.png}
        \caption{Summaries distribution}
        \label{fig:Summaries}
    \end{subfigure}
    \label{fig:distributions}
\end{figure}

\newpage
\subsection{How does the summarization system perform for the full collection? And within each category? Any intuition for the observed differences?}
The following graphs show the overall results, both on whole dataset or by class. \\
We might say that there are differences among classes, but that should be cautious since to prove it we would require a non-parametric statistic test to assert differences on MAP. We can suppose that eventual differences might be result of the different words distribution among categories,with different keywords vocabulary size. 

\begin{center}
    \begin{table}[H]
        \centering
        \begin{tabular}{|l|c|c|c|c|c|}
            \hline
            Category          & Documents & Precision & Recall & F1    & MAP   \\
            \hline
            business          & 510       & 0.642     & 0.426  & 0.512 & 0.859 \\
            entertainment     & 386       & 0.651     & 0.452  & 0.533 & 0.848 \\
            sport             & 511       & 0.669     & 0.482  & 0.561 & 0.904 \\
            politics          & 417       & 0.717     & 0.381  & 0.498 & 0.864 \\
            tech              & 401       & 0.695     & 0.347  & 0.463 & 0.810 \\
            \hline
            whole\_collection & 2225      & 0.675     & 0.421  & 0.518 & 0.992 \\
            \hline
        \end{tabular}
        \caption{Performance metrics for different categories.}
        \label{tab:performance}
    \end{table}
\end{center}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/whole_collection_performance.png}
    \caption{Precision and recall for the whole collection.}
    \label{fig:Performances_whole}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{images/class_comparison.png}
    \caption{Performances by categories}
    \label{fig:Performances_cat}
\end{figure}


\subsection{How IR models affect summaries? How vector space models compare with language models?}
The comparison between BM25 model (that performs similarly to TF-IDF) and BERT model with our algorithm tends to be in favour of this second implementation. 
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
    Metrica & BM25 & BERT \\
    \hline
    Average Precision & 0.25 & 0.30 \\
    Average Recall & 0.14 & 0.17 \\
    F1 Score & 0.18 & 0.22 \\
    Standard Deviation of Precision & 0.06 & 0.17 \\
    Standard Deviation of Recall & 0.04 & 0.15 \\
    Mean Average Precision & 0.12 & 0.86 \\
    \hline
    \end{tabular}
    \caption{Performance comparison for BM25 and BERT.}
    \label{tab:my-table}
\end{table}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/bm25_bert.png}
    \caption{Performance comparison for BM25 and BERT.}
    \label{fig:bm25_bert}
\end{figure}


\subsection{Is Reciprocal Rank Fusion (RRF) useful to aid decisions?}
We expected that RRF would help to improve the summaries, since it's a way to
combine the results of different systems and, in machine learning contexts,
ensembles are generally outperforming single models. \\ Our test was not done
on the entire collection due to the scarcity of computational power. Thus, we
conduced a test on a subset of the dataset,that didn't really show any
particular improvement on the scoring metrics. Due to this, we can say that RRF
did not improve our system, but we cannot say that it is not an useful technique in general, since the size
of our testing dataset was not significative and the BERT implementation is
based on our suppositions. Moreover, a BERT-based solution is not suggested
since the model usage times, combined with the additional overhead caused by
calculating distances in the latent space, cause a significant slower response.


\begin{center}
    \begin{table}[H]
        \centering
        \begin{tabular}{|l|c|c|c|c|c|c|}
            \hline
            Method & Avg. Prec & Avg. Rec & F1 Score & Std Prec & Std Rec & MAP    \\
            \hline
            BM25   & 0.7554    & 0.4250   & 0.5440   & 0.1962   & 0.1574  & 0.7869 \\
            \hline
            RRF    & 0.5105    & 0.2832   & 0.3643   & 0.1223   & 0.1140  & 0.5014 \\
            \hline
        \end{tabular}
        \caption{Performance metrics for BM25 and RRF.}
        \label{tab:bm25_rrf}
    \end{table}
\end{center}

\subsection{Considering MMR, how $\lambda$ impacts the accuracy (against ideal extracts) of summaries? Should $\lambda$ be a fixed threshold or depend on the provided topic document (d-specific)?}
In this case we have a strange outcome. In fact, the $\lambda$ parameter seems to not affect at all the scores. Our idea is that this is caused by the fact that we could conduct the test only on a small set of documents (20), so the summarization on them might stay unchanged for different $\lambda$s.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{images/lambda_metrics.png}
    \caption{Performances at different $\lambda$}
    \label{fig:MMR performance}    
\end{figure}
\subsection{At the suggested $p$ length threshold, is the system better at promoting recall or precision?}
We noticed that precision is overall usually higher than recall, due to the fact the system tends to avoid to put non relevant sentences in the summary, leading to a lower recall.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/pre_rec.png}
    \caption{Precision and recall at given threshold}
    \label{fig:Precision and recall}
\end{figure}